{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4c8f452",
   "metadata": {},
   "source": [
    "루브릭\n",
    "1. 번역기 모델 학습에 필요한 텍스트 데이터 전처리가 잘 이루어졌다.  \n",
    "구두점, 대소문자, 띄어쓰기 등 번역기 모델에 요구되는 전처리가 정상적으로 진행되었다.  \n",
    "2. seq2seq 기반의 번역기 모델이 정상적으로 구동된다.  \n",
    "seq2seq 모델 훈련결과를 그래프로 출력해보고, validation loss그래프가 우하향하는 경향성을 보이며 학습이 진행됨이 확인되었다.  \n",
    "3. 테스트 결과 의미가 통하는 수준의 번역문이 생성되었다.  \n",
    "테스트용 디코더 모델이 정상적으로 만들어졌으며, input(영어)와 output(프랑스어) 모두 한글로 번역해서 결과를 출력해보았고, 둘의 내용이 유사함을 확인하였다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2761a357",
   "metadata": {},
   "source": [
    "# 단어 Level 번역기 만들기\n",
    "데이터는 상위 33,000개의 샘플만 사용  \n",
    "1.정제, 정규화, 전처리  \n",
    "구두점 분리, 소문자 전환, 띄어쓰기 단위 토큰화  \n",
    "2.디코더 문장에 시작 및 종료 토큰 삽입  \n",
    "3.케라스 토크나이저로 텍스트 숫자 변환  \n",
    "4.임배딩 레이어 사용하기(노드 10-10 step 4 참조  \n",
    "5.모델 구현하기  \n",
    "sparse categorical entropy loss사용  \n",
    "6.모델 평가하기  \n",
    "  \n",
    "### 데이터 불러오기 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a6d7ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ecf26ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 197463\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Va !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Marche.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>En route !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Bouge !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eng         fra                                                 cc\n",
       "0  Go.        Va !  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
       "1  Go.     Marche.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
       "2  Go.  En route !  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
       "3  Go.     Bouge !  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
       "4  Hi.     Salut !  CC-BY 2.0 (France) Attribution: tatoeba.org #5..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = os.getenv('HOME')+'/aiffel/translator_seq2seq/data/fra.txt'\n",
    "lines = pd.read_csv(file_path, names=['eng', 'fra', 'cc'], sep='\\t')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "# lines.sample(5) #샘플 5개 출력\n",
    "lines.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8265a1f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Va !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Marche.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>En route !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Bouge !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eng         fra\n",
       "0  Go.        Va !\n",
       "1  Go.     Marche.\n",
       "2  Go.  En route !\n",
       "3  Go.     Bouge !\n",
       "4  Hi.     Salut !"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3만 3천개의 데이터만 사용한다.(train 30000, test 3000)\n",
    "lines = lines[['eng', 'fra']][:33000]\n",
    "lines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1386c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 33000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>&lt;sos&gt; Va ! &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>&lt;sos&gt; Marche. &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>&lt;sos&gt; En route ! &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Go.</td>\n",
       "      <td>&lt;sos&gt; Bouge ! &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>&lt;sos&gt; Salut ! &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>&lt;sos&gt; Salut. &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Run!</td>\n",
       "      <td>&lt;sos&gt; Cours ! &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Run!</td>\n",
       "      <td>&lt;sos&gt; Courez ! &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Run!</td>\n",
       "      <td>&lt;sos&gt; Prenez vos jambes à vos cous ! &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Run!</td>\n",
       "      <td>&lt;sos&gt; File ! &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    eng                                         fra\n",
       "0   Go.                            <sos> Va ! <eos>\n",
       "1   Go.                         <sos> Marche. <eos>\n",
       "2   Go.                      <sos> En route ! <eos>\n",
       "3   Go.                         <sos> Bouge ! <eos>\n",
       "4   Hi.                         <sos> Salut ! <eos>\n",
       "5   Hi.                          <sos> Salut. <eos>\n",
       "6  Run!                         <sos> Cours ! <eos>\n",
       "7  Run!                        <sos> Courez ! <eos>\n",
       "8  Run!  <sos> Prenez vos jambes à vos cous ! <eos>\n",
       "9  Run!                          <sos> File ! <eos>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰 추가\n",
    "sos_token = '<sos>'\n",
    "eos_token = '<eos>'\n",
    "# sos_token과 eos_token사이에 ' '를 추가하여야 토그나이저에서 문제를 일으키지 않는다.\n",
    "lines.fra = lines.fra.apply(lambda x : sos_token + ' ' + x + ' '+ eos_token)\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62691507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>go.</td>\n",
       "      <td>&lt;sos&gt; va ! &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>go.</td>\n",
       "      <td>&lt;sos&gt; marche. &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>go.</td>\n",
       "      <td>&lt;sos&gt; en route ! &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>go.</td>\n",
       "      <td>&lt;sos&gt; bouge ! &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hi.</td>\n",
       "      <td>&lt;sos&gt; salut ! &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hi.</td>\n",
       "      <td>&lt;sos&gt; salut. &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>run!</td>\n",
       "      <td>&lt;sos&gt; cours ! &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>run!</td>\n",
       "      <td>&lt;sos&gt; courez ! &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>run!</td>\n",
       "      <td>&lt;sos&gt; prenez vos jambes à vos cous ! &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>run!</td>\n",
       "      <td>&lt;sos&gt; file ! &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    eng                                         fra\n",
       "0   go.                            <sos> va ! <eos>\n",
       "1   go.                         <sos> marche. <eos>\n",
       "2   go.                      <sos> en route ! <eos>\n",
       "3   go.                         <sos> bouge ! <eos>\n",
       "4   hi.                         <sos> salut ! <eos>\n",
       "5   hi.                          <sos> salut. <eos>\n",
       "6  run!                         <sos> cours ! <eos>\n",
       "7  run!                        <sos> courez ! <eos>\n",
       "8  run!  <sos> prenez vos jambes à vos cous ! <eos>\n",
       "9  run!                          <sos> file ! <eos>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모든 대문자 소문자 변환\n",
    "lines['eng']=lines['eng'].str.lower()\n",
    "lines['fra']=lines['fra'].str.lower()\n",
    "lines.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19f5cb0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[22], [22], [22]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 영어 토큰화\n",
    "eng_tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')   # 문자 단위로 Tokenizer를 생성합니다. \n",
    "eng_tokenizer.fit_on_texts(lines.eng)               # eng의 각 행에 토큰화를 수행\n",
    "input_text = eng_tokenizer.texts_to_sequences(lines.eng)    # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "input_text[:3] # 0,1,2모두 go 기 때문에 같을 수 밖에 없으니 당황하지 말 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8152240f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 59, 2], [1, 351, 2], [1, 22, 499, 2]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 프랑스어 토큰화\n",
    "fra_tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')   # 문자 단위로 Tokenizer를 생성합니다. \n",
    "fra_tokenizer.fit_on_texts(lines.fra)                 # fra의 각 행에 토큰화를 수행\n",
    "target_text = fra_tokenizer.texts_to_sequences(lines.fra)     # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "target_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0f29b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 토큰 중 디코더 입력 데이터에 종료 토큰과 디코더 target 데이터에 시작 토큰 제거\n",
    "encoder_input = input_text\n",
    "# 종료 토큰 제거\n",
    "decoder_input = [[ char for char in line if char != fra_tokenizer.word_index[eos_token] ] for line in target_text] \n",
    "# 시작 토큰 제거\n",
    "decoder_target = [[ char for char in line if char != fra_tokenizer.word_index[sos_token] ] for line in target_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47590ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 최대 길이: 6\n",
      "프랑스어 최대 길이: 14\n"
     ]
    }
   ],
   "source": [
    "# 패딩 길이 확인을 위해 영어와 프랑스어 시퀸스의 최대 길이를 구한다.(단어 단위)\n",
    "max_eng_seq_len=max([len(k_len) for k_len in input_text])\n",
    "max_fra_seq_len=max([len(f_len) for f_len in target_text])\n",
    "print(\"영어 최대 길이:\", max_eng_seq_len)\n",
    "print(\"프랑스어 최대 길이:\", max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbc27326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (33000, 6)\n",
      "프랑스어 입력데이터의 크기(shape) : (33000, 14)\n",
      "프랑스어 출력데이터의 크기(shape) : (33000, 14)\n"
     ]
    }
   ],
   "source": [
    "encoder_input= pad_sequences(encoder_input, maxlen=max_eng_seq_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen = max_fra_seq_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen = max_fra_seq_len, padding='post')\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f963e75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd14ac0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어장의 크기 : 4799\n",
      "프랑스어 단어장의 크기 : 10009\n"
     ]
    }
   ],
   "source": [
    "# 각 단어장 크기 확인\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f61badf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_input = to_categorical(encoder_input)\n",
    "# decoder_input = to_categorical(decoder_input)\n",
    "# decoder_target = to_categorical(decoder_target)\n",
    "# print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "# print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "# print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a59911af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 학습데이터의 크기(shape) : (33000, 6)\n",
      "프랑스어 학습 입력데이터의 크기(shape) : (33000, 14)\n",
      "프랑스어 학습 출력데이터의 크기(shape) : (33000, 14)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(30000, 6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 노드와 마찬가지로 3000건만 검증데이터로 준다.\n",
    "n_of_val = 3000\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('영어 학습데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 학습 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 학습 출력데이터의 크기(shape) :',np.shape(decoder_target))\n",
    "\n",
    "np.shape(encoder_input_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b2e1345",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633bfdaa",
   "metadata": {},
   "source": [
    "### 모델 형성 및 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d94f4b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None,))\n",
    "# 단어장의 크기 및 임베딩 벡터 차원입력\n",
    "enc_emb =  Embedding(4799, 8)(encoder_inputs)\n",
    "encoder_lstm = LSTM(units=256, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9a1e465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 입력 텐서 생성.(교사 강요)\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "# 단어장의 크기 및 임베딩 벡터 차원입력\n",
    "fra_emb= Embedding(10009, 8)(decoder_inputs)\n",
    "decoder_lstm = LSTM(units=256, return_sequences = True, return_state=True)\n",
    "decoder_outputs, _, _= decoder_lstm(fra_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd200c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 중 가장 확률이 높은 하나의 단어만 선택\n",
    "decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e34eee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2d5866f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 8)      80072       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  271360      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 10009)  2572313     lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,923,745\n",
      "Trainable params: 2,923,745\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77a3da8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "235/235 [==============================] - 7s 19ms/step - loss: 2.5212 - val_loss: 2.2431\n",
      "Epoch 2/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.7302 - val_loss: 1.9863\n",
      "Epoch 3/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.6052 - val_loss: 1.9105\n",
      "Epoch 4/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.5489 - val_loss: 1.8731\n",
      "Epoch 5/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.5105 - val_loss: 1.8504\n",
      "Epoch 6/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.4796 - val_loss: 1.8468\n",
      "Epoch 7/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.4503 - val_loss: 1.8122\n",
      "Epoch 8/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.4243 - val_loss: 1.7960\n",
      "Epoch 9/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.4025 - val_loss: 1.7762\n",
      "Epoch 10/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.3839 - val_loss: 1.7738\n",
      "Epoch 11/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.3675 - val_loss: 1.7601\n",
      "Epoch 12/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.3538 - val_loss: 1.7618\n",
      "Epoch 13/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.3396 - val_loss: 1.7528\n",
      "Epoch 14/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.3259 - val_loss: 1.7532\n",
      "Epoch 15/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.3139 - val_loss: 1.7408\n",
      "Epoch 16/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.3030 - val_loss: 1.7476\n",
      "Epoch 17/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.2927 - val_loss: 1.7290\n",
      "Epoch 18/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.2822 - val_loss: 1.7260\n",
      "Epoch 19/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.2732 - val_loss: 1.7335\n",
      "Epoch 20/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.2642 - val_loss: 1.7329\n",
      "Epoch 21/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.2558 - val_loss: 1.7327\n",
      "Epoch 22/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.2457 - val_loss: 1.7264\n",
      "Epoch 23/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.2368 - val_loss: 1.7151\n",
      "Epoch 24/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.2280 - val_loss: 1.7116\n",
      "Epoch 25/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.2200 - val_loss: 1.7206\n",
      "Epoch 26/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.2135 - val_loss: 1.7203\n",
      "Epoch 27/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.2075 - val_loss: 1.7157\n",
      "Epoch 28/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.2023 - val_loss: 1.7224\n",
      "Epoch 29/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.1974 - val_loss: 1.7175\n",
      "Epoch 30/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.1926 - val_loss: 1.7150\n",
      "Epoch 31/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.1879 - val_loss: 1.7174\n",
      "Epoch 32/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.1826 - val_loss: 1.7147\n",
      "Epoch 33/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.1779 - val_loss: 1.7196\n",
      "Epoch 34/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.1739 - val_loss: 1.7103\n",
      "Epoch 35/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.1688 - val_loss: 1.7139\n",
      "Epoch 36/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.1642 - val_loss: 1.7225\n",
      "Epoch 37/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.1591 - val_loss: 1.7222\n",
      "Epoch 38/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.1545 - val_loss: 1.7251\n",
      "Epoch 39/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.1498 - val_loss: 1.7251\n",
      "Epoch 40/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.1452 - val_loss: 1.7201\n",
      "Epoch 41/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.1416 - val_loss: 1.7236\n",
      "Epoch 42/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.1388 - val_loss: 1.7236\n",
      "Epoch 43/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.1362 - val_loss: 1.7265\n",
      "Epoch 44/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.1337 - val_loss: 1.7263\n",
      "Epoch 45/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.1311 - val_loss: 1.7395\n",
      "Epoch 46/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.1286 - val_loss: 1.7324\n",
      "Epoch 47/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.1262 - val_loss: 1.7456\n",
      "Epoch 48/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.1242 - val_loss: 1.7389\n",
      "Epoch 49/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.1220 - val_loss: 1.7532\n",
      "Epoch 50/50\n",
      "235/235 [==============================] - 4s 17ms/step - loss: 1.1189 - val_loss: 1.7527\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f20f47e91c0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size=128, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d7f519",
   "metadata": {},
   "source": [
    "### 모델 테스트하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3369435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 8)           38392     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  [(None, 256), (None, 256) 271360    \n",
      "=================================================================\n",
      "Total params: 309,752\n",
      "Trainable params: 309,752\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(inputs = encoder_inputs, outputs = [state_h, state_c])\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05eac310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전 time step의 hidden state를 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "# 이전 time step의 cell state를 저장하는 텐서\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "# 이전 time step의 hidden state와 cell state를 하나의 변수에 저장\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# decoder_states_inputs를 현재 time step의 초기 상태로 사용.\n",
    "# decoder_lstm에 fra_emb 적용.\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(fra_emb, initial_state = decoder_states_inputs)\n",
    "# 현재 time step의 hidden state와 cell state를 하나의 변수에 저장.\n",
    "decoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3ec05e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 8)      80072       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  271360      embedding_1[0][0]                \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 10009)  2572313     lstm_1[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,923,745\n",
      "Trainable params: 2,923,745\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1989a1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng2idx = eng_tokenizer.word_index\n",
    "fra2idx = fra_tokenizer.word_index\n",
    "idx2eng = eng_tokenizer.index_word\n",
    "idx2fra = fra_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "389c2bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음(리스트)\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성(3차원 matrix)\n",
    "    target_seq = np.zeros((1, 1, fra_vocab_size))\n",
    "    target_seq[0, 0, fra2idx['<sos>']] = 1.\n",
    "    \n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = idx2fra[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '<eos>' or\n",
    "           len(decoded_sentence) > max_fra_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1, fra_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "32c2548b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, None) for input KerasTensor(type_spec=TensorSpec(shape=(None, None), dtype=tf.float32, name='input_2'), name='input_2', description=\"created by layer 'input_2'\"), but it was called on an input with incompatible shape (None, 1, 10009).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:1586 predict_function  *\n        return step_function(self, iterator)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:1576 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:1569 run_step  **\n        outputs = model.predict_step(data)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:1537 predict_step\n        return self(x, training=False)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py:1037 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/functional.py:414 call\n        return self._run_internal_graph(\n    /opt/conda/lib/python3.9/site-packages/keras/engine/functional.py:550 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/keras/layers/recurrent.py:716 __call__\n        return super(RNN, self).__call__(inputs, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py:1020 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/input_spec.py:214 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer lstm_1 is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 1, 10009, 8)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1796/893521880.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1001\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# 입력 문장의 인덱스 (자유롭게 선택해 보세요)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0minput_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mseq_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdecoded_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m35\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'입력 문장:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meng\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1796/2587950757.py\u001b[0m in \u001b[0;36mdecode_sequence\u001b[0;34m(input_seq)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstop_condition\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0moutput_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_seq\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstates_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# 예측 결과를 문자로 변환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1749\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1751\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1752\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3036\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m-> 3038\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3457\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3458\u001b[0m               call_context_key in self._function_cache.missed):\n\u001b[0;32m-> 3459\u001b[0;31m             return self._define_function_with_shape_relaxation(\n\u001b[0m\u001b[1;32m   3460\u001b[0m                 args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[0;34m(self, args, kwargs, flat_args, filtered_flat_args, cache_key_context)\u001b[0m\n\u001b[1;32m   3379\u001b[0m           expand_composites=True)\n\u001b[1;32m   3380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3381\u001b[0;31m     graph_function = self._create_graph_function(\n\u001b[0m\u001b[1;32m   3382\u001b[0m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[1;32m   3383\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3296\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3297\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3298\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3299\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3300\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:1586 predict_function  *\n        return step_function(self, iterator)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:1576 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:1569 run_step  **\n        outputs = model.predict_step(data)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:1537 predict_step\n        return self(x, training=False)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py:1037 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/functional.py:414 call\n        return self._run_internal_graph(\n    /opt/conda/lib/python3.9/site-packages/keras/engine/functional.py:550 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/keras/layers/recurrent.py:716 __call__\n        return super(RNN, self).__call__(inputs, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py:1020 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/input_spec.py:214 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer lstm_1 is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 1, 10009, 8)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for seq_index in [3,50,100,300,1001]: # 입력 문장의 인덱스 (자유롭게 선택해 보세요)\n",
    "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.eng[seq_index])\n",
    "    print('정답 문장:', lines.fra[seq_index][1:len(lines.fra[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dfc5bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
